{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6255c34-6aea-4087-811c-cfbd6aefeddf",
   "metadata": {},
   "source": [
    "# Gen Dataset from huggingface -- UltraFeedback\n",
    "\n",
    "1. Get the dataset from huggingface, convert it to jsonl files\n",
    "2. Generate corresponding different responses using API, merge them into the probing dataset.\n",
    "\n",
    "Generate the following files:\n",
    "\n",
    "- *train_chosen.jsonl*: ['prompt': prompt, 'chosen': chosen example]\n",
    "- *train_reject.jsonl*: ['prompt': prompt, 'chosen': reject example]\n",
    "- *train_dpo.jsonl*: ['prompt': prompt, 'chosen': chosen example, 'reject': rejected example]\n",
    "- *prob_train.jsonl*: ['prompt': prompt, 'y1': y1, 'y2': y2, ...], all from train set\n",
    "- *prob_test.jsonl*: ['prompt': prompt, 'y1': y1, 'y2': y2, ...], all from test set\n",
    "\n",
    "1. Chosen (best in 4, sfted)\n",
    "     - 1.1 Initial response\n",
    "     - 1.2 Self-rephrase\n",
    "     - 1.3 GPT-rephrase, semantics keeping\n",
    "     - 1.4 GPT-rephrase, format keeping\n",
    "2. Rejected (worst in 4)\n",
    "     - 2.1 GPT-rephrase, semantics keeping\n",
    "     - 2.2 GPT-rephrase, format keeping\n",
    "3. Irrelavent in train (j neq i)\n",
    "4. Irrelavent in test (j neq i)\n",
    "5. Irrelavent random HumLang Passage\n",
    "6. Randomly permute chosen tokens\n",
    "7. Pure random tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde6bcbe-e787-4b5c-bdce-04afbe17ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List, Optional, Iterator, Callable, Union, Tuple\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import sys \n",
    "#sys.path.append(\"../..\")\n",
    "#from preference_datasets import extract_anthropic_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f7dd9b1a-3afb-4c0f-95ab-4f3fe1310597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    response_list = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            response_list.append(json.loads(line))\n",
    "    return response_list\n",
    "\n",
    "def extract_ultrafb_prompt(orig_prompt):\n",
    "    return f'Human: {orig_prompt}\\nAssistant:'\n",
    "\n",
    "def get_selfr_prompt(chosen):\n",
    "    chosen_r = f'Please rephrase the following sentence: \\n{chosen}\\nRephrase: '\n",
    "    return chosen_r\n",
    "\n",
    "def gen_perm_response(response):\n",
    "    response_words = response.strip().split(' ')\n",
    "    len_response = len(response_words)\n",
    "    random.shuffle(response_words)\n",
    "    permuted_response = \" \".join(str(element) for element in response_words)\n",
    "    return permuted_response\n",
    "\n",
    "def get_model_response_init(prompt, idx, model_response):\n",
    "    #assert model_response[idx]['prompt'] == prompt\n",
    "    response = model_response[idx]['response'][len(prompt):].strip(' ').split('Human')[0]  # Only select the first part of the response\n",
    "    if len(response)<=1:\n",
    "        return 'Opps, the model cannot generate correct response.'\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "def get_model_response_selfr(chosen_response, idx, model_response):\n",
    "    response_ = model_response[idx]['response'].split('\\nRephrase: ')\n",
    "    if len(response_)==1:\n",
    "        return 'Opps, the model cannot generate correct response.'\n",
    "    else:\n",
    "        return response_[1]\n",
    "\n",
    "def get_gpt_response(tmp, idx, chosen_or_rej='chosen', res_type='chosen_gptsemantic', gpt_response=None):\n",
    "    if res_type=='irr_hum':\n",
    "        return gpt_response[idx][f'{res_type}'].strip()\n",
    "    assert gpt_response[idx][f'{chosen_or_rej}'].strip()==tmp.strip()\n",
    "    return gpt_response[idx][f'{res_type}'].strip()\n",
    "\n",
    "# ------------- Generate train_chosen and train_reject\n",
    "def get_original_ultra(split: str = None):\n",
    "    \"\"\"Get the original ultrafeedback dataset.\n",
    "    Args:\n",
    "        split: the split of the dataset to be loaded.\n",
    "    Returns:\n",
    "        dataset: the original HH dataset.\n",
    "    \"\"\"\n",
    "    print(f'Loading ultrafeedback dataset')\n",
    "    if split is None:\n",
    "        dataset = datasets.load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")\n",
    "    else:\n",
    "        dataset = datasets.load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=split)\n",
    "    print('done')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def convert_ultrafeedback(dataset: List[Dict[str, str]], dict_type='chosen', n_samples=-1, model='pythia2.8b',train_or_test='train') -> List[Dict[str, str]]:\n",
    "    \"\"\"Convert the UltraFB dataset to the format described above.\n",
    "    Args:\n",
    "        dataset: the original UltraFB subset.\n",
    "        dict_type: which dictionary we would formulate\n",
    "            - train_chosen\n",
    "            - train_reject\n",
    "            - train_dpo\n",
    "            - prob\n",
    "    Returns:\n",
    "        converted_dataset: the converted dataset.\n",
    "    \"\"\"\n",
    "    if dict_type=='prob':\n",
    "        model_response_initial = read_jsonl(os.path.join('qwen_generated_responses', model, f'prob_{train_or_test}_gen_response.jsonl'))\n",
    "        model_response_selfr = read_jsonl(os.path.join('qwen_generated_responses', model, f'prob_{train_or_test}_selfr_response.jsonl'))\n",
    "        gpt_response = read_jsonl(os.path.join('gpt_generated_responses',f'gpt_3.5_{train_or_test}.jsonl'))   \n",
    "    converted_dataset = []\n",
    "    if n_samples < 0:\n",
    "        N_SAMPLE = len(dataset)\n",
    "    else:\n",
    "        N_SAMPLE = n_samples\n",
    "    for idx in trange(0, N_SAMPLE):\n",
    "        ex = dataset[idx]\n",
    "        orig_prompt = ex['prompt']\n",
    "        prompt = extract_ultrafb_prompt(ex['prompt'])\n",
    "        if dict_type == 'chosen':\n",
    "            chosen_response = ex['chosen'][1]['content']\n",
    "            converted_dataset.append({'prompt': prompt, 'chosen': chosen_response,})\n",
    "        elif dict_type == 'reject':\n",
    "            chosen_response = ex['rejected'][1]['content']\n",
    "            converted_dataset.append({'prompt': prompt, 'chosen': chosen_response,})\n",
    "        elif dict_type == 'dpo':\n",
    "            chosen_response = ex['chosen'][1]['content']\n",
    "            rejected_response = ex['rejected'][1]['content']\n",
    "            converted_dataset.append({'prompt': prompt, 'chosen': chosen_response,'rejected': rejected_response,})\n",
    "        elif dict_type == 'dpo_reverse':\n",
    "            # ----------- change the role of rej and chosen\n",
    "            chosen_response = ex['chosen'][1]['content']\n",
    "            rejected_response = ex['rejected'][1]['content']\n",
    "            converted_dataset.append({'prompt': prompt, 'chosen': rejected_response,'rejected': chosen_response,})           \n",
    "        elif dict_type == 'selfr':\n",
    "            chosen_response = ex['chosen'][1]['content']\n",
    "            selfr_prompt = get_selfr_prompt(chosen_response)\n",
    "            converted_dataset.append({'prompt': selfr_prompt, 'chosen': ' ','rejected': ' '})    \n",
    "        elif dict_type == 'prob':\n",
    "            # ------ Chosen group: 'chosen', 'chosen_initial', 'chosen_selfr', 'chosen_gptsemantic', 'chosen_gptformat'\n",
    "            chosen_response = ex['chosen'][1]['content']\n",
    "            chosen_initial_response = get_model_response_init(prompt, idx, model_response_initial)\n",
    "            chosen_selfr_response = get_model_response_selfr(chosen_response, idx, model_response_selfr)\n",
    "            chosen_gptsemantic_res = get_gpt_response(chosen_response, idx, 'chosen', 'chosen_gptsemantic', gpt_response)\n",
    "            chosen_gptformat_res = get_gpt_response(chosen_response, idx, 'chosen', 'chosen_gptformat', gpt_response)\n",
    "            # ------ Reject group: 'reject', 'reject_gptsemantic', 'reject_gptformat'\n",
    "            rejected_response = ex['rejected'][1]['content']\n",
    "            reject_gptsemantic_res = get_gpt_response(rejected_response, idx, 'rejected', 'rejected_gptsemantic', gpt_response)\n",
    "            reject_gptformat_res = get_gpt_response(rejected_response, idx, 'rejected', 'rejected_gptformat', gpt_response)\n",
    "            \n",
    "            # ------ Irrelavent group: 'irr_train', 'irr_test', 'irr_hum'\n",
    "            while True:\n",
    "                irr_train_idx = random.randint(0, N_SAMPLE)\n",
    "                if irr_train_idx != idx:\n",
    "                    break\n",
    "            irr_test_idx = random.randint(N_SAMPLE, len(dataset))\n",
    "            irr_train_prompt = extract_ultrafb_prompt(dataset[irr_train_idx]['chosen'])\n",
    "            irr_test_prompt = extract_ultrafb_prompt(dataset[irr_test_idx]['chosen'])\n",
    "            irr_train_response = dataset[irr_train_idx]['chosen'][1]['content']\n",
    "            irr_test_response = dataset[irr_test_idx]['chosen'][1]['content']\n",
    "            irr_hum_res = get_gpt_response(rejected_response, idx, 'rejected', 'irr_hum', gpt_response)    \n",
    "            \n",
    "            # ------ Random Group: 'random_permute', 'random_nonhum'      \n",
    "            rnd_perm_response = gen_perm_response(chosen_response)\n",
    "            rnd_nonhum_response = gen_perm_response(irr_test_response)   \n",
    "            \n",
    "            converted_dataset.append({\n",
    "                'prompt': prompt,\n",
    "                'chosen': chosen_response,\n",
    "                'chosen_initial': chosen_initial_response,\n",
    "                'chosen_selfr': chosen_selfr_response,\n",
    "                'chosen_gptsemantic':chosen_gptsemantic_res,\n",
    "                'chosen_gptformat':chosen_gptformat_res,\n",
    "                'rejected': rejected_response,\n",
    "                'reject_gptsemantic': reject_gptsemantic_res,\n",
    "                'reject_gptformat': reject_gptformat_res,\n",
    "                'irr_train': irr_train_response,\n",
    "                'irr_test': irr_test_response,\n",
    "                'irr_hum': irr_hum_res,\n",
    "                'random_permute' : rnd_perm_response,\n",
    "                'random_nonhum' : rnd_nonhum_response,\n",
    "            })\n",
    "            \n",
    "    return converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "305cfccf-6c44-439e-bea3-e363cf460e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ultrafeedback dataset\n",
      "done\n",
      "Loading ultrafeedback dataset\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_original_data = get_original_ultra('train_prefs')\n",
    "test_original_data = get_original_ultra('test_prefs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c41d4e9-7849-449b-8b62-9640cd8abca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5914.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# -------- Standard dpo training dataset\n",
    "DATA_NAME = \"train_dpo\"\n",
    "converted = convert_ultrafeedback(train_original_data, 'dpo',n_samples=5000)\n",
    "with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd630aa-9446-42ba-b6bf-d97b92bfcff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = \"test_dpo\"\n",
    "converted = convert_ultrafeedback(test_original_data, 'dpo',n_samples=500)\n",
    "with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cfd64464-c428-4e19-a14e-8aadcb4cc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Create another train_sft_extend using existing train_dpo.json\n",
    "# -------- Use for the proposed method: first SFT with y+ and y- together\n",
    "DATA_NAME = \"train_sft_extend\"\n",
    "\n",
    "with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as fw:\n",
    "    with open('E://P5_5_SFT_dynamics//finetuning_dynamics//data//ultrafeedback//train_dpo.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            prompt = json.loads(line)['prompt']\n",
    "            chosen = json.loads(line)['chosen']\n",
    "            rejected = json.loads(line)['rejected']\n",
    "            fw.write(json.dumps({'prompt': prompt, 'chosen': chosen}))\n",
    "            fw.write('\\n')\n",
    "            fw.write(json.dumps({'prompt': prompt, 'chosen': rejected}))\n",
    "            fw.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd07851-7b09-45d8-be67-0595b2371cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 3616.97it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 6124.50it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_NAME = \"prob_train_selfr\"\n",
    "converted = convert_ultrafeedback(train_original_data, 'selfr',n_samples=500)\n",
    "with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')\n",
    "\n",
    "DATA_NAME = \"prob_test_selfr\"\n",
    "converted = convert_ultrafeedback(test_original_data, 'selfr',n_samples=500)\n",
    "with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b266e2-f7d2-41c8-b418-ac29f54ab2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 5192.05it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 6410.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# -------- Standard dpo training dataset\n",
    "DATA_NAME = \"prob_train_gen\"\n",
    "converted = convert_ultrafeedback(train_original_data, 'chosen',n_samples=500)\n",
    "with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')\n",
    "\n",
    "DATA_NAME = \"prob_test_gen\"\n",
    "converted = convert_ultrafeedback(test_original_data, 'chosen',n_samples=500)\n",
    "with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c3d0d9a1-76fc-4da4-a7d7-8496d3ecb4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 550.65it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 819.82it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_NAME = \"formal_prob_train\"\n",
    "MODEL = 'qwen18'\n",
    "converted = convert_ultrafeedback(train_original_data, 'prob',n_samples=500,  model=MODEL, train_or_test='train')\n",
    "with open(f'{DATA_NAME}.jsonl', 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')\n",
    "\n",
    "DATA_NAME = \"formal_prob_test\"\n",
    "MODEL = 'qwen18'\n",
    "converted = convert_ultrafeedback(test_original_data, 'prob',n_samples=500,  model=MODEL, train_or_test='test')\n",
    "with open(f'{DATA_NAME}.jsonl', 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee19d47-a4d7-4d26-8368-4e75f1713566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1803ce-342d-4e89-abc5-3bf839defe38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9e5466f-26aa-4855-8381-4cfbbf303318",
   "metadata": {},
   "source": [
    "## Generate rephrase using GPT, save them into jsonl file\n",
    "\n",
    "{'prompt':[], 'chosen':[], 'chosen_gptsemantic':[], 'chosen_gptformat':[], 'rejected':[], 'reject_gptsemantic':[], 'reject_gptformat':[], 'irr_hum':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba37965c-f4c7-4147-825b-06830ac674d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_rephrase_prompt_generator(ref_sentence=None, keep_type='gptsemantic', length=10):\n",
    "    if keep_type=='gptsemantic':\n",
    "        rephrase_prompt = f'Given the reference sentence, please generate an output sentence. \\\n",
    "Please use different words as much as possible while keeping the meaning of the reference sentence unchanged. \\\n",
    "Please only return the output sentence.\\n\\n\\\n",
    "Reference sentence: {ref_sentence}\\n\\n\\\n",
    "Output: '\n",
    "    elif keep_type=='gptformat':\n",
    "        rephrase_prompt = f'Given the reference sentence, please generate an output sentence. \\\n",
    "Please change the meaning of the reference sentence as much as possible while keeping the format of it. \\\n",
    "Please only return the output sentence.\\n\\n\\\n",
    "Reference sentence: {ref_sentence}\\n\\n\\\n",
    "Output: ' \n",
    "    elif keep_type=='irr_hum':\n",
    "        rephrase_prompt = 'Please generate a random sentence with %d words.'%length\n",
    "    else:\n",
    "        rephrase_prompt = 'There is no response from gpt.'\n",
    "    return rephrase_prompt\n",
    "\n",
    "def get_gpt_completion(ref_sentence=None, keep_type='gptsemantic', length=10):\n",
    "    sys_msg = [{\"role\": 'system', \"content\": 'You are a helpful assistant.'}]\n",
    "    rephrase_prompt = gpt_rephrase_prompt_generator(ref_sentence, keep_type=keep_type, length=length)\n",
    "    response = GPT_client.chat.completions.create(\n",
    "                        model = 'gpt-3.5-turbo',\n",
    "                        messages = sys_msg + [{'role': 'user', 'content': rephrase_prompt}],\n",
    "                        temperature=0.7,\n",
    "                        top_p = 1,         \n",
    "                        logprobs = False,\n",
    "                        )\n",
    "    gpt_response = response.choices[0].message.content    \n",
    "    \n",
    "    # if keep_type=='gptsemantic':\n",
    "    #     gpt_response = 'test gptsemantic'\n",
    "    # elif keep_type=='gptformat':\n",
    "    #     gpt_response = 'gptformat test'\n",
    "    # elif keep_type=='irr_hum':\n",
    "    #     gpt_response = length\n",
    "    return gpt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6fdd7c3d-83f7-4739-8db5-26701657358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-20-30-40-50-60-70-80-90-100-110-120-130-140-150-160-170-180-190-200-210-220-230-240-250-260-270-280-290-300-310-320-330-340-350-360-370-380-390-400-410-420-430-440-450-460-470-480-490-500-"
     ]
    }
   ],
   "source": [
    "GPT_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "train_or_test = 'train'#'test' #\n",
    "response = []\n",
    "cnt = 0\n",
    "\n",
    "# if os.path.exists(f'gpt_generated_responses/gpt_3.5_{train_or_test}.jsonl'):\n",
    "#     os.remove(f'gpt_generated_responses/gpt_3.5_{train_or_test}.jsonl')\n",
    "\n",
    "with open(f'{train_or_test}_dpo.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        cnt+=1\n",
    "        if cnt%10==0:\n",
    "            print(cnt,end='-')\n",
    "        if cnt==501:\n",
    "            break\n",
    "        prompt = json.loads(line)['prompt'].strip()\n",
    "        # ------ chosen_gptsemantic and chosen_gptformat\n",
    "        chosen = json.loads(line)['chosen'].strip()\n",
    "        chosen_gptsemantic_response = get_gpt_completion(ref_sentence=chosen, keep_type='gptsemantic')\n",
    "        chosen_gptformat_response = get_gpt_completion(ref_sentence=chosen, keep_type='gptformat')\n",
    "\n",
    "        # ------ reject_gptsemantic and reject_gptformat\n",
    "        rejected = json.loads(line)['rejected'].strip()\n",
    "        rejected_gptsemantic_response = get_gpt_completion(ref_sentence=rejected, keep_type='gptsemantic')\n",
    "        rejected_gptformat_response = get_gpt_completion(ref_sentence=rejected, keep_type='gptformat')\n",
    "\n",
    "        irr_hum_response = get_gpt_completion(keep_type='irr_hum', length=len(chosen.split(' ')))\n",
    "        \n",
    "        with open(f'gpt_generated_responses/gpt_3.5_{train_or_test}.jsonl', 'a') as f:\n",
    "            f.write(json.dumps({'prompt':prompt, \n",
    "                    'chosen':chosen, 'chosen_gptsemantic':chosen_gptsemantic_response, 'chosen_gptformat':chosen_gptformat_response,\n",
    "                    'rejected':rejected, 'rejected_gptsemantic':rejected_gptsemantic_response, 'rejected_gptformat':rejected_gptformat_response,\n",
    "                     'irr_hum':irr_hum_response}))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8369a0c5-7865-4d6e-bb4e-3beeb4564de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece9992-4232-4c57-af0b-a1176347b911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e463870-e3ee-4041-94db-c4b1b98bb86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480b5ed-d625-4885-8930-7ba8e9cfaa75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad99fc-e5c4-4c2d-9546-fe81f7e22a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
