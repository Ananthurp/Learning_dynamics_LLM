{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6255c34-6aea-4087-811c-cfbd6aefeddf",
   "metadata": {},
   "source": [
    "# Gen Dataset from huggingface\n",
    "\n",
    "1. Get the dataset from huggingface, convert it to jsonl files\n",
    "2. Generate corresponding different responses using API, merge them into the probing dataset.\n",
    "\n",
    "Generate the following files:\n",
    "\n",
    "- *train_chosen.jsonl*: ['prompt': prompt, 'chosen': chosen example]\n",
    "- *train_reject.jsonl*: ['prompt': prompt, 'chosen': reject example]\n",
    "- *train_dpo.jsonl*: ['prompt': prompt, 'chosen': chosen example, 'reject': rejected example]\n",
    "- *prob_train.jsonl*: ['prompt': prompt, 'y1': y1, 'y2': y2, ...], all from train set\n",
    "- *prob_test.jsonl*: ['prompt': prompt, 'y1': y1, 'y2': y2, ...], all from test set\n",
    "\n",
    "1. Chosen (best in 4, sfted)\n",
    "     - 1.1 Initial response\n",
    "     - 1.2 Self-rephrase\n",
    "     - 1.3 GPT-rephrase, semantics keeping\n",
    "     - 1.4 GPT-rephrase, format keeping\n",
    "2. Rejected (worst in 4)\n",
    "     - 2.1 GPT-rephrase, semantics keeping\n",
    "     - 2.2 GPT-rephrase, format keeping\n",
    "3. Irrelavent in train (j neq i)\n",
    "4. Irrelavent in test (j neq i)\n",
    "5. Irrelavent random HumLang Passage\n",
    "6. Randomly permute chosen tokens\n",
    "7. Pure random tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde6bcbe-e787-4b5c-bdce-04afbe17ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "from typing import Dict, List, Optional, Iterator, Callable, Union, Tuple\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "from preference_datasets import extract_anthropic_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7dd9b1a-3afb-4c0f-95ab-4f3fe1310597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    response_list = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            response_list.append(json.loads(line))\n",
    "    return response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cbfb814-fd17-40aa-9349-d1fd91e3292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Generate train_chosen and train_reject\n",
    "def get_original_helpful(split: str = None):\n",
    "    \"\"\"Get the original HH dataset.\n",
    "    Args:\n",
    "        split: the split of the dataset to be loaded.\n",
    "    Returns:\n",
    "        dataset: the original HH dataset.\n",
    "    \"\"\"\n",
    "    print(f'Loading HH dataset (helpful-base split) from Huggingface...')\n",
    "    if split is None:\n",
    "        dataset = datasets.load_dataset(\n",
    "            'Anthropic/hh-rlhf', data_dir='helpful-base', cache_dir='./data'\n",
    "        )\n",
    "    else:\n",
    "        dataset = datasets.load_dataset(\n",
    "            'Anthropic/hh-rlhf', data_dir='helpful-base', cache_dir='./data',\n",
    "            split=split\n",
    "        )\n",
    "    print('done')\n",
    "    return dataset\n",
    "\n",
    "def gen_perm_response(response):\n",
    "    response_words = response.strip().split(' ')\n",
    "    len_response = len(response_words)\n",
    "    random.shuffle(response_words)\n",
    "    permuted_response = \" \".join(str(element) for element in response_words)\n",
    "    return permuted_response\n",
    "\n",
    "def get_model_response_init(prompt, idx, model_response):\n",
    "    assert model_response[idx]['prompt'] == prompt\n",
    "    response = model_response[idx]['response'][len(prompt):]\n",
    "    search_term = '\\n\\nHuman:'\n",
    "    search_term_idx  = response.find(search_term)\n",
    "    return response[:search_term_idx]\n",
    "\n",
    "def get_model_response_selfr(chosen_response, idx, model_response):\n",
    "    assert model_response[idx]['prompt'].split('\\n@@@ Original Sentence: ')[1].split('\\n@@@ Rephrase: ')[0]==chosen_response.strip()\n",
    "    return model_response[idx]['response'].split('\\n@@@ Rephrase: ')[-1].strip()\n",
    "\n",
    "def get_gpt_response(tmp, idx, chosen_or_rej='chosen', res_type='chosen_gptsemantic', gpt_response=None):\n",
    "    if res_type=='irr_hum':\n",
    "        return gpt_response[idx][f'{res_type}'].strip()\n",
    "    assert gpt_response[idx][f'{chosen_or_rej}'].strip()==tmp.strip()\n",
    "    return gpt_response[idx][f'{res_type}'].strip()\n",
    "\n",
    "def convert_helpful_base(dataset: List[Dict[str, str]], dict_type='chosen', n_samples=-1, model='pythia2.8b',train_or_test='train') -> List[Dict[str, str]]:\n",
    "    \"\"\"Convert the HH dataset to the format described above.\n",
    "    Args:\n",
    "        dataset: the original HH helpful-base subset.\n",
    "        dict_type: which dictionary we would formulate\n",
    "            - train_chosen\n",
    "            - train_reject\n",
    "            - train_dpo\n",
    "            - prob\n",
    "    Returns:\n",
    "        converted_dataset: the converted dataset.\n",
    "    \"\"\"\n",
    "    if dict_type=='prob':\n",
    "        model_response_initial = read_jsonl(os.path.join('pythia_generated_responses', model, f'prob_{train_or_test}_gen_response.jsonl'))\n",
    "        model_response_selfr = read_jsonl(os.path.join('pythia_generated_responses', model, f'prob_{train_or_test}_selfr_response.jsonl'))\n",
    "        gpt_response = read_jsonl(os.path.join('gpt_generated_responses',f'gpt_3.5_{train_or_test}.jsonl'))\n",
    "    converted_dataset = []\n",
    "    if n_samples < 0:\n",
    "        N_SAMPLE = len(dataset)\n",
    "    else:\n",
    "        N_SAMPLE = n_samples\n",
    "    for idx in trange(0, N_SAMPLE):\n",
    "        ex = dataset[idx]\n",
    "        prompt = extract_anthropic_prompt(ex['chosen'])\n",
    "        if dict_type == 'chosen':\n",
    "            chosen_response = ex['chosen'][len(prompt):]\n",
    "            converted_dataset.append({'prompt': prompt, 'chosen': chosen_response,})\n",
    "        elif dict_type == 'reject':\n",
    "            chosen_response = ex['rejected'][len(prompt):]\n",
    "            converted_dataset.append({'prompt': prompt, 'chosen': chosen_response,})\n",
    "        elif dict_type == 'dpo':\n",
    "            chosen_response = ex['chosen'][len(prompt):]\n",
    "            rejected_response = ex['rejected'][len(prompt):]\n",
    "            converted_dataset.append({'prompt': prompt, 'chosen': chosen_response,'rejected': rejected_response,})\n",
    "        elif dict_type == 'dpo_reverse':\n",
    "            # ----------- change the role of rej and chosen\n",
    "            chosen_response = ex['chosen'][len(prompt):]\n",
    "            rejected_response = ex['rejected'][len(prompt):]\n",
    "            converted_dataset.append({'prompt': prompt, 'chosen': rejected_response,'rejected': chosen_response,})\n",
    "        elif dict_type == 'prob':\n",
    "            # ------ Chosen group: 'chosen', 'chosen_initial', 'chosen_selfr', 'chosen_gptsemantic', 'chosen_gptformat'\n",
    "            chosen_response = ex['chosen'][len(prompt):]\n",
    "            chosen_initial_response = get_model_response_init(prompt, idx, model_response_initial)\n",
    "            chosen_selfr_response = get_model_response_selfr(chosen_response, idx, model_response_selfr)\n",
    "            chosen_gptsemantic_res = get_gpt_response(chosen_response, idx, 'chosen', 'chosen_gptsemantic', gpt_response)\n",
    "            chosen_gptformat_res = get_gpt_response(chosen_response, idx, 'chosen', 'chosen_gptformat', gpt_response)\n",
    "            # ------ Reject group: 'reject', 'reject_gptsemantic', 'reject_gptformat'\n",
    "            rejected_response = ex['rejected'][len(prompt):]\n",
    "            reject_gptsemantic_res = get_gpt_response(rejected_response, idx, 'rejected', 'rejected_gptsemantic', gpt_response)\n",
    "            reject_gptformat_res = get_gpt_response(rejected_response, idx, 'rejected', 'rejected_gptformat', gpt_response)\n",
    "\n",
    "            # ------ Irrelavent group: 'irr_train', 'irr_test', 'irr_hum'\n",
    "            while True:\n",
    "                irr_train_idx = random.randint(0, N_SAMPLE)\n",
    "                if irr_train_idx != idx:\n",
    "                    break\n",
    "            irr_test_idx = random.randint(N_SAMPLE, len(dataset))\n",
    "            irr_train_prompt = extract_anthropic_prompt(dataset[irr_train_idx]['chosen'])\n",
    "            irr_test_prompt = extract_anthropic_prompt(dataset[irr_test_idx]['chosen'])\n",
    "            irr_train_response = dataset[irr_train_idx]['chosen'][len(irr_train_prompt):]\n",
    "            irr_test_response = dataset[irr_test_idx]['chosen'][len(irr_test_prompt):]\n",
    "            irr_hum_res = get_gpt_response(rejected_response, idx, 'rejected', 'irr_hum', gpt_response)\n",
    "            \n",
    "            # ------ Random Group: 'random_permute', 'random_nonhum'\n",
    "            rnd_perm_response = gen_perm_response(chosen_response)\n",
    "            rnd_nonhum_response = gen_perm_response(irr_test_response)\n",
    "            \n",
    "            converted_dataset.append({\n",
    "                'prompt': prompt,\n",
    "                'chosen': chosen_response,\n",
    "                'chosen_initial': chosen_initial_response,\n",
    "                'chosen_selfr': chosen_selfr_response,\n",
    "                'chosen_gptsemantic':chosen_gptsemantic_res,\n",
    "                'chosen_gptformat':chosen_gptformat_res,\n",
    "                'rejected': rejected_response,\n",
    "                'reject_gptsemantic': reject_gptsemantic_res,\n",
    "                'reject_gptformat': reject_gptformat_res,\n",
    "                'irr_train': irr_train_response,\n",
    "                'irr_test': irr_test_response,\n",
    "                'irr_hum': irr_hum_res,\n",
    "                'random_permute' : rnd_perm_response,\n",
    "                'random_nonhum' : rnd_nonhum_response,\n",
    "            })\n",
    "    return converted_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c16df-dc2d-426c-b5dd-8472d142a397",
   "metadata": {},
   "source": [
    "## Generate train and test dpo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baed928f-605d-40ab-a293-1d79f50d5f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████▉                                                               | 6963/43835 [18:42<1:39:05,  6.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m cl, rl \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_original_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen\u001b[39m\u001b[38;5;124m'\u001b[39m]))):\n\u001b[1;32m----> 4\u001b[0m     cl\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_original_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchosen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i]))\n\u001b[0;32m      5\u001b[0m     rl\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(train_original_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m'\u001b[39m][i]))\n",
      "File \u001b[1;32m~\\.conda\\envs\\alignment\\lib\\site-packages\\datasets\\arrow_dataset.py:2780\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\alignment\\lib\\site-packages\\datasets\\arrow_dataset.py:2765\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2764\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2765\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2767\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32m~\\.conda\\envs\\alignment\\lib\\site-packages\\datasets\\formatting\\formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32m~\\.conda\\envs\\alignment\\lib\\site-packages\\datasets\\formatting\\formatting.py:405\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[1;32m~\\.conda\\envs\\alignment\\lib\\site-packages\\datasets\\formatting\\formatting.py:448\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m--> 448\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "cl, rl = [], []\n",
    "for i in tqdm(range(len(train_original_data['chosen']))):\n",
    "    cl.append(len(train_original_data['chosen'][i]))\n",
    "    rl.append(len(train_original_data['rejected'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24ba269f-25af-4ab9-98dc-fef40c4c39de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789.7475226195605 575.0602016758355\n",
      "732.42926899325 571.3383131150688\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cl), np.std(cl))\n",
    "print(np.mean(rl), np.std(rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7ead4-93f0-4387-ae59-5c319fc11f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9cb5ef2-4d3e-4103-ab0e-bb8725073f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HH dataset (helpful-base split) from Huggingface...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6126e9f798314333a26fa0a204da5fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YIREN\\.conda\\envs\\alignment\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\YIREN\\.cache\\huggingface\\hub\\datasets--Anthropic--hh-rlhf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Loading HH dataset (helpful-base split) from Huggingface...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_original_data = get_original_helpful('train')\n",
    "test_original_data = get_original_helpful('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a788c074-2e91-4d9e-96a8-b4f6875d4958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 15507.87it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_NAME = \"train_dpo\"\n",
    "converted = convert_helpful_base(train_original_data, 'dpo',n_samples=5000)\n",
    "with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eea48ee0-c107-4733-8680-43e008c97db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Create another train_sft_extend using existing train_dpo.json\n",
    "DATA_NAME = \"train_sft_extend\"\n",
    "\n",
    "with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as fw:\n",
    "    with open('E://P5_5_SFT_dynamics//finetuning_dynamics//data//helpful-base//train_dpo.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            prompt = json.loads(line)['prompt']\n",
    "            chosen = json.loads(line)['chosen']\n",
    "            rejected = json.loads(line)['rejected']\n",
    "            fw.write(json.dumps({'prompt': prompt, 'chosen': chosen}))\n",
    "            fw.write('\\n')\n",
    "            fw.write(json.dumps({'prompt': prompt, 'chosen': rejected}))\n",
    "            fw.write('\\n')\n",
    "            \n",
    "\n",
    "# with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as f:\n",
    "#     for i in range(len(converted)):  \n",
    "#         f.write(json.dumps(converted[i]))\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e5b55-5e27-4396-9ec3-dab987d32db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c370db22-06cd-4d8c-9a3b-b1a9c04fbf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 15671.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# DATA_NAME = \"train_dpo_reverse\"\n",
    "# converted = convert_helpful_base(train_original_data, 'dpo_reverse',n_samples=5000)\n",
    "# with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as f:\n",
    "#     for i in range(len(converted)):  \n",
    "#         f.write(json.dumps(converted[i]))\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41f932ba-3b53-4bed-8a76-3b746b70dafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 9406.76it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_NAME = \"test_dpo\"\n",
    "converted = convert_helpful_base(test_original_data, 'dpo',n_samples=500)\n",
    "with open('%s.jsonl'%DATA_NAME, 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fcfab47e-53b1-437b-acc0-a618c3d839d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 16020.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# ------------- Use this to generate full probing data\n",
    "DATA_NAME = \"prob_train_gen\"\n",
    "converted = convert_helpful_base(train_original_data, 'chosen',n_samples=500)\n",
    "with open('%s.jsonl'%DATA_NAME, 'w') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ac8fad0e-c475-4fbb-9be8-5d35dbd4b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 14893.70it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_NAME = \"prob_test_gen\"\n",
    "converted = convert_helpful_base(test_original_data, 'chosen',n_samples=500)\n",
    "with open('%s.jsonl'%DATA_NAME, 'w') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab8b28a-c09a-497e-a64f-04afbe721c02",
   "metadata": {},
   "source": [
    "### Generating different responses for the prob dataset\n",
    "\n",
    "1. For *chosen_initial* and *chosen_selfr*, we need to run xxx.py to generate the output, and save the results in xx.json\n",
    "2. For *gpt* responses and *irr_hum*, we need API's feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef6c8d8f-df35-4a0d-9d21-1f4bca0265cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1434.20it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2617.45it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_NAME = \"formal_prob_train\"\n",
    "MODEL = 'pythia2.8b'\n",
    "converted = convert_helpful_base(train_original_data, 'prob',n_samples=500,  model=MODEL, train_or_test='train')\n",
    "with open(f'{DATA_NAME}.jsonl', 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')\n",
    "\n",
    "DATA_NAME = \"formal_prob_test\"\n",
    "MODEL = 'pythia2.8b'\n",
    "converted = convert_helpful_base(test_original_data, 'prob',n_samples=500,  model=MODEL, train_or_test='test')\n",
    "with open(f'{DATA_NAME}.jsonl', 'w', newline='\\n') as f:\n",
    "    for i in range(len(converted)):  \n",
    "        f.write(json.dumps(converted[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813a251-7176-48b1-b963-1bff33d268a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ebbc3-020b-41e5-80c4-4c1884d96e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ee7f4-3be3-4ae5-9032-9fa15e5086e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "574be715-0c57-4669-a9dc-58876e4d4eff",
   "metadata": {},
   "source": [
    "## Generate rephrase using GPT, save them into jsonl file\n",
    "\n",
    "{'prompt':[], 'chosen':[], 'chosen_gptsemantic':[], 'chosen_gptformat':[], 'rejected':[], 'reject_gptsemantic':[], 'reject_gptformat':[], 'irr_hum':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79c7478b-e48f-4eb9-9ace-26c20f50a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_rephrase_prompt_generator(ref_sentence=None, keep_type='gptsemantic', length=10):\n",
    "    if keep_type=='gptsemantic':\n",
    "        rephrase_prompt = f'Given the reference sentence, please generate an output sentence. \\\n",
    "Please use different words as much as possible while keeping the meaning of the reference sentence unchanged. \\\n",
    "Please only return the output sentence.\\n\\n\\\n",
    "Reference sentence: {ref_sentence}\\n\\n\\\n",
    "Output: '\n",
    "    elif keep_type=='gptformat':\n",
    "        rephrase_prompt = f'Given the reference sentence, please generate an output sentence. \\\n",
    "Please change the meaning of the reference sentence as much as possible while keeping the format of it. \\\n",
    "Please only return the output sentence.\\n\\n\\\n",
    "Reference sentence: {ref_sentence}\\n\\n\\\n",
    "Output: ' \n",
    "    elif keep_type=='irr_hum':\n",
    "        rephrase_prompt = 'Please generate a random sentence with %d words.'%length\n",
    "    else:\n",
    "        rephrase_prompt = 'There is no response from gpt.'\n",
    "    return rephrase_prompt\n",
    "\n",
    "def get_gpt_completion(ref_sentence=None, keep_type='gptsemantic', length=10):\n",
    "    sys_msg = [{\"role\": 'system', \"content\": 'You are a helpful assistant.'}]\n",
    "    rephrase_prompt = gpt_rephrase_prompt_generator(ref_sentence, keep_type=keep_type, length=length)\n",
    "    response = GPT_client.chat.completions.create(\n",
    "                        model = 'gpt-3.5-turbo',\n",
    "                        messages = sys_msg + [{'role': 'user', 'content': rephrase_prompt}],\n",
    "                        temperature=0.7,\n",
    "                        top_p = 1,         \n",
    "                        logprobs = False,\n",
    "                        )\n",
    "    gpt_response = response.choices[0].message.content    \n",
    "    \n",
    "    # if keep_type=='gptsemantic':\n",
    "    #     gpt_response = 'test gptsemantic'\n",
    "    # elif keep_type=='gptformat':\n",
    "    #     gpt_response = 'gptformat test'\n",
    "    # elif keep_type=='irr_hum':\n",
    "    #     gpt_response = length\n",
    "    return gpt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b6c7ac6-4f44-422f-859b-7d8fdec3cf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-20-30-40-50-60-70-80-90-100-110-120-130-140-150-160-170-180-190-200-210-220-230-240-250-260-270-280-290-300-310-320-330-340-350-360-370-380-390-400-410-420-430-440-450-460-470-480-490-500-"
     ]
    }
   ],
   "source": [
    "GPT_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "train_or_test = 'test' #'train'#\n",
    "response = []\n",
    "cnt = 0\n",
    "# if os.path.exists(f'gpt_generated_responses/gpt_3.5_{train_or_test}.jsonl'):\n",
    "#     os.remove(f'gpt_generated_responses/gpt_3.5_{train_or_test}.jsonl')\n",
    "with open(f'{train_or_test}_dpo.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        cnt+=1\n",
    "        if cnt%10==0:\n",
    "            print(cnt,end='-')\n",
    "        if cnt==501:\n",
    "            break\n",
    "        prompt = json.loads(line)['prompt'].strip()\n",
    "        # ------ chosen_gptsemantic and chosen_gptformat\n",
    "        chosen = json.loads(line)['chosen'].strip()\n",
    "        chosen_gptsemantic_response = get_gpt_completion(ref_sentence=chosen, keep_type='gptsemantic')\n",
    "        chosen_gptformat_response = get_gpt_completion(ref_sentence=chosen, keep_type='gptformat')\n",
    "\n",
    "        # ------ reject_gptsemantic and reject_gptformat\n",
    "        rejected = json.loads(line)['rejected'].strip()\n",
    "        rejected_gptsemantic_response = get_gpt_completion(ref_sentence=rejected, keep_type='gptsemantic')\n",
    "        rejected_gptformat_response = get_gpt_completion(ref_sentence=rejected, keep_type='gptformat')\n",
    "\n",
    "        irr_hum_response = get_gpt_completion(keep_type='irr_hum', length=len(chosen.split(' ')))\n",
    "        \n",
    "        with open(f'gpt_generated_responses/gpt_3.5_{train_or_test}.jsonl', 'a') as f:\n",
    "            f.write(json.dumps({'prompt':prompt, \n",
    "                    'chosen':chosen, 'chosen_gptsemantic':chosen_gptsemantic_response, 'chosen_gptformat':chosen_gptformat_response,\n",
    "                    'rejected':rejected, 'rejected_gptsemantic':rejected_gptsemantic_response, 'rejected_gptformat':rejected_gptformat_response,\n",
    "                     'irr_hum':irr_hum_response}))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e539a4a-ae29-4325-a4ee-2e28c81e4474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
