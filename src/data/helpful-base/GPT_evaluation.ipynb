{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1e4bf8-6f9b-41f0-897d-7db0c4f52db1",
   "metadata": {},
   "source": [
    "# GPT evaluation on responses generated by 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857f6b17-4196-4af1-b9c3-f6eaa43762f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, List, Optional, Iterator, Callable, Union, Tuple\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "EVALUATOR = 'claude' #'gpt'# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99522f6a-ac97-48e4-9ded-1cb5a3ab9863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_evaluation_prompt_generator(prompt, response_A, response_B):\n",
    "    prompt = f'Given the history of multi-round chat, which response is more helpful?\\n\\n\\\n",
    "History:\\n{prompt}\\n\\n\\\n",
    "Response A: {response_A}\\n\\n\\\n",
    "Response B: {response_B}\\n\\n\\\n",
    "FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful. \\\n",
    "SECOND, on a new line, state only \"A\" or \"B\" to indicate which response is more helpful. \\\n",
    "Your response should use the format:\\n\\n\\\n",
    "Comparison: <one-sentence comparison and explanation>\\n\\\n",
    "More helpful: <\"A\" or \"B\">'\n",
    "    return prompt\n",
    "\n",
    "def GPT_completion(msg_to_gpt, temp=0.7):\n",
    "    if EVALUATOR == 'gpt':\n",
    "        sys_msg = [{\"role\": 'system', \"content\": 'You are a helpful assistant.'}]\n",
    "        GPT_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        response = GPT_client.chat.completions.create(\n",
    "                            model = 'gpt-3.5-turbo',\n",
    "                            messages = sys_msg + [{'role': 'user', 'content': msg_to_gpt}],\n",
    "                            temperature=temp,\n",
    "                            top_p = 1,         \n",
    "                            logprobs = False,\n",
    "                            )\n",
    "        gpt_response = response.choices[0].message.content    \n",
    "        return gpt_response\n",
    "    elif EVALUATOR == 'claude':\n",
    "        sys_msg = [{\"role\": 'system', \"content\": 'You are a helpful assistant.'}]\n",
    "        Claude_client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "        response = Claude_client.messages.create(\n",
    "                            model = 'claude-3-haiku-20240307',\n",
    "                            system = 'You are a helpful assistant.',\n",
    "                            messages = [{'role': 'user', 'content': msg_to_gpt}],\n",
    "                            temperature=temp,\n",
    "                            max_tokens=2048,\n",
    "                            top_p = 1,\n",
    "                            )\n",
    "        claude_response = response.content[0].text   \n",
    "        return claude_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7463d9a2-d05a-4962-b30a-e31a4a5cf347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the history of multi-round chat, which response is more helpful?\n",
      "\n",
      "History:\n",
      "hello world\n",
      "\n",
      "Response A: test A\n",
      "\n",
      "Response B: test B\n",
      "\n",
      "FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful. SECOND, on a new line, state only \"A\" or \"B\" to indicate which response is more helpful. Your response should use the format:\n",
      "\n",
      "Comparison: <one-sentence comparison and explanation>\n",
      "More helpful: <\"A\" or \"B\">\n"
     ]
    }
   ],
   "source": [
    "print(gpt_evaluation_prompt_generator(prompt='hello world', response_A='test A', response_B='test B'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cdd21a-ec8b-447d-b395-ffa1e1118b8b",
   "metadata": {},
   "source": [
    "## Extract responses from the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa7966f-48f2-4be1-8369-7b975353b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'E://P5_5_SFT_dynamics//finetuning_dynamics//data//helpful-base//methods_compare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f798a2cb-3795-417c-b7c0-46bf91102396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrac_dict(exp_name):\n",
    "    response_dict = {\"prompt\":[], \"response\":[]}\n",
    "    with open(os.path.join(PATH, exp_name, 'prob_test_gen_response.jsonl'), 'r') as f:\n",
    "        for line in f:\n",
    "            prompt = json.loads(line)['prompt'].strip('\\n')\n",
    "            tmp_response = json.loads(line)['response'].strip('\\n')\n",
    "            if prompt in tmp_response:\n",
    "                response = tmp_response.split(prompt)[1].strip(' ')\n",
    "            else:\n",
    "                response = tmp_response.split('Assistant: ')[1]\n",
    "            response_dict[\"prompt\"].append(prompt)\n",
    "            response_dict[\"response\"].append(response)\n",
    "    return response_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5347a4-0a6c-488a-a5e8-9e54c748ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_A = 'extend_dpo_qwen18_ep4' #'extend_dpo_qwen18' #'extend_sft_qwen18' #\n",
    "MODEL_B = 'extend_dpo_qwen18_ep2' # 'baseline_dpo_qwen18' # 'baseline_sft_qwen18'\n",
    "resA_all = extrac_dict(exp_name=MODEL_A)\n",
    "resB_all = extrac_dict(exp_name=MODEL_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b07345-ef0e-412f-9d03-16919550e4bd",
   "metadata": {},
   "source": [
    "## Feed the response to GPT one-by-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26e6dba5-af8c-43f4-aae1-982faaa29712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 231/500 [04:47<05:34,  1.24s/it]\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m eval_prompt \u001b[38;5;241m=\u001b[39m gpt_evaluation_prompt_generator(prompt\u001b[38;5;241m=\u001b[39mprompt, response_A\u001b[38;5;241m=\u001b[39mresA, response_B\u001b[38;5;241m=\u001b[39mresB)\n\u001b[1;32m---> 18\u001b[0m GPT_response \u001b[38;5;241m=\u001b[39m \u001b[43mGPT_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg_to_gpt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ---------- Record and extact GPT_response\u001b[39;00m\n\u001b[0;32m     20\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  #======== Question \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt[:\u001b[38;5;241m40\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m, in \u001b[0;36mGPT_completion\u001b[1;34m(msg_to_gpt, temp)\u001b[0m\n\u001b[0;32m     27\u001b[0m sys_msg \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m'\u001b[39m}]\n\u001b[0;32m     28\u001b[0m Claude_client \u001b[38;5;241m=\u001b[39m anthropic\u001b[38;5;241m.\u001b[39mAnthropic(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANTHROPIC_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 29\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mClaude_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclaude-3-haiku-20240307\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg_to_gpt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m claude_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext   \n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m claude_response\n",
      "File \u001b[1;32m~\\.conda\\envs\\icl\\lib\\site-packages\\anthropic\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\icl\\lib\\site-packages\\anthropic\\resources\\messages.py:678\u001b[0m, in \u001b[0;36mMessages.create\u001b[1;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[0;32m    677\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message \u001b[38;5;241m|\u001b[39m Stream[MessageStreamEvent]:\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\icl\\lib\\site-packages\\anthropic\\_base_client.py:1208\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1196\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1205\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1206\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1207\u001b[0m     )\n\u001b[1;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\icl\\lib\\site-packages\\anthropic\\_base_client.py:897\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    890\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    895\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\icl\\lib\\site-packages\\anthropic\\_base_client.py:988\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    985\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    991\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    992\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    995\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    996\u001b[0m )\n",
      "\u001b[1;31mInternalServerError\u001b[0m: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}"
     ]
    }
   ],
   "source": [
    "LEN = len(resA_all['prompt'])\n",
    "eval_log_file = f\"{MODEL_A}_vs_{MODEL_B}_{EVALUATOR}.txt\"\n",
    "with open(os.path.join(PATH, 'results_logs', eval_log_file), 'a') as f:\n",
    "    f.write(f\"####### We are doing an evaluation between [{MODEL_A}] and [{MODEL_B}] #######\\n\")\n",
    "    \n",
    "A_win_cnt = 0\n",
    "B_win_cnt = 0\n",
    "all_cnt = 0\n",
    "with open(os.path.join(PATH, 'results_logs', eval_log_file), 'a', encoding='utf-8') as f:\n",
    "    for i in tqdm(range(LEN)):\n",
    "        assert resA_all['prompt'][i] == resB_all['prompt'][i]\n",
    "        prompt = resA_all['prompt'][i]\n",
    "        resA = resA_all['response'][i]\n",
    "        resB = resB_all['response'][i]\n",
    "        if len(resA) > 2000 or len(resB) > 2000:\n",
    "            continue\n",
    "        eval_prompt = gpt_evaluation_prompt_generator(prompt=prompt, response_A=resA, response_B=resB)\n",
    "        GPT_response = GPT_completion(msg_to_gpt=eval_prompt)\n",
    "        # ---------- Record and extact GPT_response\n",
    "        f.write(f'  #======== Question {i}: {prompt[:40]}\\n')\n",
    "        f.write(f'  #======== GPT eval: \\n {GPT_response}\\n')\n",
    "        GPT_answer = GPT_response.split(': ')[-1]\n",
    "        all_cnt += 1\n",
    "        if GPT_answer == 'A':\n",
    "            A_win_cnt += 1\n",
    "        if GPT_answer == 'B':\n",
    "            B_win_cnt += 1\n",
    "    f.write(f'#======== Model A win {A_win_cnt}/{all_cnt} times, model B win {B_win_cnt}/{all_cnt} ==========\\n')\n",
    "    f.write('################### EVALUATION ENDS #################\\n')\n",
    "wra = (A_win_cnt)/(A_win_cnt+B_win_cnt)\n",
    "wrb = (B_win_cnt)/(A_win_cnt+B_win_cnt)\n",
    "print(f'#======== Model A win {A_win_cnt}/{all_cnt} times, model B win {B_win_cnt}/{all_cnt}, A win {wra}, B win {wrb} ==========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "559bc25d-dae4-4a6e-8945-9dca8b20ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#======== Model A win 119/227 times, model B win 108/227 ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'#======== Model A win {A_win_cnt}/{all_cnt} times, model B win {B_win_cnt}/{all_cnt} ==========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8974e9d-194c-48b8-aeb2-0dfc2fa67a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "wra = (A_win_cnt)/(A_win_cnt+B_win_cnt)\n",
    "wrb = (B_win_cnt)/(A_win_cnt+B_win_cnt)\n",
    "print(f'#======== Model A win {A_win_cnt}/{all_cnt} times, model B win {B_win_cnt}/{all_cnt}, A win {wra}, B win {wrb} ==========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544882c9-8aad-4f08-aa2f-78b310d0d7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
